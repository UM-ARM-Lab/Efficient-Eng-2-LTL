{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"raw/ltl.txt\", \"r\") as f:\n",
    "    raw_ltls = f.read().splitlines()\n",
    "    unique_ltls = set(raw_ltls)\n",
    "with open(\"raw/eng.txt\", 'r') as f:\n",
    "    raw_engs = f.read().splitlines()\n",
    "\n",
    "DPs = []\n",
    "for ltl, eng in zip(raw_ltls, raw_engs):\n",
    "    DPs.append({'ltl': ltl, 'eng': eng})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "G & U S ! C F C\n",
      "G & U S ! A F A\n",
      "G & U S ! Y F Y\n",
      "G & U S ! R F R\n",
      "G & U S ! B F B\n"
     ]
    }
   ],
   "source": [
    "for ltl in sorted(unique_ltls, key=lambda x: len(x)):\n",
    "    print(ltl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clean-up Domain, with augmentation\n",
      "Number of Data Points 744\n",
      "Number of unique LTL expressions: 5\n",
      "Number of unique LTL structures: 1\n"
     ]
    }
   ],
   "source": [
    "print(\"Clean-up Domain, with augmentation\")\n",
    "print(\"Number of Data Points\", len(DPs))\n",
    "print(\"Number of unique LTL expressions:\", len(unique_ltls))\n",
    "print(\"Number of unique LTL structures:\", 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "G & U S ! A F A\n",
      "move the arm to the block and pick it up next placing it in the bin and coming back to pick up the next block\n",
      "\n",
      "G & U S ! R F R\n",
      "watch for blocks to be set down and then move them into the basket but not any orange ones\n",
      "\n",
      "G & U S ! B F B\n",
      "watch for blocks to be set down and then move them into the basket but not any blue ones\n",
      "\n",
      "G & U S ! C F C\n",
      "watch for blocks to be set down and then move them into the basket but not any green ones\n",
      "\n",
      "G & U S ! Y F Y\n",
      "watch for blocks to be set down and then move them into the basket but not any yellow ones\n",
      "\n"
     ]
    }
   ],
   "source": [
    "seen = set()\n",
    "from random import shuffle, seed\n",
    "seed(0)\n",
    "# shuffle(DPs)\n",
    "for dp in DPs:\n",
    "    if dp['ltl'] not in seen:\n",
    "        seen.add(dp['ltl'])\n",
    "        print(dp['ltl'])\n",
    "        print(dp['eng'])\n",
    "        print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean up LTL expressions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "APs = [\n",
    "    {'ap': \"A\", 'eng': \"any cubes\"},\n",
    "    {'ap': \"R\", 'eng': \"any non red cubes\"},\n",
    "    {'ap': \"B\", 'eng': \"any non blue cubes\"},\n",
    "    {'ap': \"Y\", 'eng': \"any non yellow cubes\"},\n",
    "    {'ap': \"C\", 'eng': \"any non green cubes\"},\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_type_A(ap1):\n",
    "    return {\n",
    "        'raw_ltl': f\"G & U S ! {ap1['ap']} F {ap1['ap']}\",\n",
    "        'canonical_ltl': f\"globally ( and ( until ( scan , not ( {ap1['eng']} ) ) , finally ( {ap1['eng']} ) ) )\",\n",
    "        'eng': f\"Look for and pick up {ap1['eng']} and put them in crate.\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "translation_seeds = []\n",
    "for r1 in APs:\n",
    "    translation_seeds.append(build_type_A(r1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n"
     ]
    }
   ],
   "source": [
    "print(len(translation_seeds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "set()\n"
     ]
    }
   ],
   "source": [
    "seed_ltls = set([t['raw_ltl'] for t in translation_seeds])\n",
    "print(unique_ltls - seed_ltls)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save the canonical decoding list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "possible_decodings = {}\n",
    "for seed in translation_seeds:\n",
    "    canonical = seed['canonical_ltl']\n",
    "    possible_decodings[canonical] = {\n",
    "        'formula': canonical,\n",
    "        'raw': seed['raw_ltl'],\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"canonical.json\", 'w') as f:\n",
    "    f.write(json.dumps(possible_decodings, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save translation seed for zero shot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"train_seed.jsonl\", \"w\") as f:\n",
    "    for dp in translation_seeds:\n",
    "        better_ltl = dp['canonical_ltl']\n",
    "        entry = {'canonical': better_ltl, 'formula': better_ltl, 'natural': dp['eng']}\n",
    "        json.dump(entry, f)\n",
    "        f.write('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save golden data for evaluation  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_canonical_mapping = {\n",
    "    seed['raw_ltl']: seed['canonical_ltl'] for seed in translation_seeds\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "744"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(DPs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"golden.jsonl\", \"w\") as f:\n",
    "    for dp in DPs:\n",
    "        entry = {\n",
    "            'canonical': raw_canonical_mapping[dp['ltl']],\n",
    "            'formula': raw_canonical_mapping[dp['ltl']],\n",
    "            'natural': dp['eng'],\n",
    "            'raw_ltl': dp['ltl'],\n",
    "        }\n",
    "        json.dump(entry, f)\n",
    "        f.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('GPML')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "92a84f77637d1d47b588cbbaac9b07f8c628b67f58e672e955ed4902878afbbe"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
